# googlenet

> Going deeper with convolutions

## Abstract

这种架构的主要特点是提高了网络内部计算资源的利用率。

允许增加网络的深度和宽度，同时保持计算预算不变。

为了优化质量，架构决策基于Hebbian原则和多尺度处理的intuition。

## Introduction

参数变多，准确率变高。

在目标检测方面，最大的收获不是来自于仅仅利用深度网络或更大的模型，而是来自于深度架构和经典计算机视觉的协同，比如R-CNN算法

架构考虑了算法的效率，堆内存和功率的使用。

## Related Work

一般大数据集都采用更深的架构和更大的卷积核，同时用dropout来防止过拟合的问题。

最大池化可能会导致图像精确信息的丢失。

重复使用了多次Inception块，网络也达到了比较深的层数。

NIN网络是为了提高神经网络的表征能力。googlenet大量使用了该方法。主要用于：1.降维；2.以在不影响性能的情况下增加网络深度。

R-CNN将整体检测问题分为两个子问题：先利用低维特征（比如颜色）对于以不知道其类别的方式搜索潜在的目标，然后使用CNN在这些位置进行分类。googlenet采用了这种方法但进行了改进，比如使用multi-box。

## Motivation and High Level Considerations

在数据量大的情况下，增加模型深度和增大卷积核宽度是两个有效的方法去增加模型的能力。但是会有更多的参数，使过拟合的情况更容易发生，特别是数据量较少的时候。这极大的增加了计算资源的占用（当有的权重被计算至0时，相当于浪费掉该参数）。

上述两个问题的解决的一个办法是用稀疏连接代替完全连接，甚至在卷积内部也这样。

目前大多数面向视觉的机器学习系统都是利用卷积来利用空间稀疏性的。然而，卷积是作为与前一层patch的密集连接的集合来实现的。

## Architectural Details

Inception体系结构的卷积核大小分别为1×1、3×3和5×5。

这也意味着，建议的架构是所有这些层的组合，它们的输出滤波器组连接成一个单一的输出向量，形成下一阶段的输入。

每个阶段添加一个可选择的并行池化路径应该会有额外的有益效果

因为可能会有大量的参数需要计算，所以可以算则进行降维和投影。比如embedding，即使低维嵌入也可能会包含大量关于相对较大的图像的信息。

在3 * 3和5 * 5 的卷积核之前使用1 * 1的卷积核用来减少参数，也用来当作线性激活函数。

## GoogleNet

从完全连接层变成平均池化层会将top-1的准确率提高约0.6%，然而，即使在删除完全连接的层之后，dropout的使用仍然是必不可少的。

通过添加连接到这些中间层的辅助分类器，我们可以期望在分类器的较低阶段进行分类，增加传播回来的梯度信号，并提供额外的正则化。

使用softmax层做分类。

![googlenet](/pic/googlenet.png)



## Training Methodology

对图像的不同大小的块进行采样，这些块的大小均匀分布在图像面积的8%到100%之间，其宽高比随机选择在3/4到4/3之间。

## Conclusions

## Acknowledgements



