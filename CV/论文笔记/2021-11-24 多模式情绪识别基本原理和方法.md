# Emotion Recognition from Multiple Modalities: Fundamentals and Methodologies
> 多模式情绪识别:基本原理和方法

## Abstract

谈论了几种有代表性的方法：每种情感模式的表征学习、不同情感模式的特征融合、MER分类器优化和MER域适配。并讨论了未来的方向。

## Introduction

multi-modal emotion recognition (MER,多模态情感识别)

多模态数据的优点：1.数据互补；2.模型鲁棒性；3.性能较好。

## psychological models

两种有代表性的情感评估模型：分类情感状态(CES，categorical emotion states)和维度情感空间(DES，dimensional emotion space)。

## affective modalities

* A. Explicit Affective Cues

面部表情、眼神、语言、肢体动作、步态、脑电图（electroencephalogram，EEG）

* B. Implicit Affective Stimuli

文本、音频、图片、视频

## data collections and emotion annotations

构建mer数据集通常需要两步：1.数据收集和情感标注

## computational tasks

* A. Emotion Classification

  目前大多数的情感分类任务都是给数据分配单个标签（a single label learning，SLL），而情绪可能是不同区域或序列的混合表达，而不仅仅是由单一情绪表示的。多模态学习（multi label learning，MLL）用来研究一个实例与多个情绪标签关联的问题。

* B. Emotion Regression

  学习一个映射函数可以有效地将一个实例与笛卡尔空间中连续的情绪值相关联。

  MER最常见的回归算法旨在将平均维度值分配给源数据。

  用分布在dimensional valence arousal（VA）的连续概率分布来表达情绪内在的特征。VA情感标签可以由Gaussian mixture model（GMM）表示，从而将情绪预测转化为参数学习问题。

* C. Emotion Detection

  找到某种情绪在源文件中的位置。将情感和相应区域匹配。

* D. Emotion Retrieval（情感检索）

  根据人的感知来搜索情感内容。

## challenges

* A. Affective Gap

  提取具有鉴别能力的高级特征，特别是与那些情绪相关的特征。

* B. Perception Subjectivity

  人的情感因素具有主观性，不同的人对同一场景可能会有不同的情感。

* C. Data Incompleteness

  数据不完整会带来影响。

* D. Cross-modality Inconsistency

  同一样本的不同模式可能会相互冲突，从而表达不同的情绪。

* E. Cross-modality Imbalance

  不同方式对情感倾向带来的贡献不同。

* F. Label Noise And Absence

  需要设计有效的无监督学习或者弱监督学习（unsupervised/weakly supervised learning）和few/zero shot learning。

* 我们可能在一个领域有足够的标记情感数据，另一个问题是如何将训练好的MER模型从一个有标签的domain转移到一个没有标签的domain。

* 当使用直接传输时，域移位的存在会导致显著的性能下降。多模态域自适应和域泛化可以有效地减小这种域间隙。还应该考虑实际的设置，例如多个源域。

![feature-fusion](/pic/feature-fusion.jpg)

## computational methodologies

MER框架中由三部分：表示学习、特征融合和分类器优化

* A. Representation Learning of Each Affective Modality

  将文本表示出张量：onehot、word2vec、glove、bert、xlnet

  文本特征可以有词向量表示，用RNN对单词顺序进行建模。

  音频通常转化成图像表示，然后用CNN提取特征。

  可以从CNN的多个层中提取特征，利用注意力机制学习特定的局部情感区域。

  使用三维CNN来提取视频特征。

  可以使用三维坐标系表示人体的每个关节来提取步态表征，用LSTM或GCN来表示步态的高层次情感表征。

  将CNN、RNN等端到端的深度学习神经网络应用于原始脑电图信号，得到强大的深度特征，使用空间注意力机制来提取更多判别空间的信息。

* B. Feature Fusion of Different Affective Modalities

  有两种融合策略:无模型融合和基于模型融合

  * **Model-free fusion：**不依赖于特定的学习算法，可以分为：早期融合、晚期融合和混合融合。
    * 早期融合又称为特征级融合，直接连接来自不同形式的特征表示作为一个单一的表示。
    * 晚期融合也成为决策级融合，综合单一模态的预测结果。比如平均、投票、信号方差。
    * 混合融合就是将两者结合起来。
  * **Model-based fusion：**
    * 对于浅层模型，核融合（如SVM，对于不同的模态，使用不同的核）和图融合（为每个模态构造单独的图或超图）是两种典型的融合方法。
    * 目前流行的深度模型有基于神经网络的融合、基于注意力的融合和基于张量的融合。注意力机制有平行共注意法和交替共注意法。最近设计了一直多模态自适应门（Multimodal Adaptation Gate, MAG），可以将信息的多模态映射为一个具有轨迹和大小的矢量。
    * 对于基于模型的融合，除了基于核的融合，其他的都可以用于时间建模，如基于图的融合方法使用隐马尔可夫模型(HMM)和条件随机场(CRF)，基于神经网络的融合使用RNN和LSTM网络。

![特征融合3](/pic/特征融合3.jpg)

* C. Classifier Optimization for Multi-modal Emotion Recogni-
  tion

  CNN和RNN都可以用来处理以单词嵌入序列表示的文本。近年来更多的是使用BERT或GPT-3，他们包含多头自注意力网络。与RNN相比，不再需要对字符顺序进行处理，transform可以模拟较远单词之间的关系。

  在音频情感识别领域，最近开发出了深度混合卷积和循环模型。

  对于视频来说，使用三维CNN来提取时空特征，最近的方法提出了极性一致交叉熵损失（polarity-consistent cross-entropy loss）来指导注意的产生。

  ST-GCN可以用来进行步态情绪预测。

![特征融合2](/pic/特征融合2.jpg)

* D. Domain Adaptation for Multi-modal Emotion Recognition

  可以使用差异损失或鉴别器来校准融合的特征表示。不同模态之间的对应可以被用作一种自我监督的校准。

## application

~~**不想写**~~

## future direction

需要解决的问题：1.如何平衡普通情绪反应和个性化情绪反应；2.以及如何强调更重要的模式

### New Methodologies for MER

1)背景知识和先验知识建模；2)从未标记、不可靠和不匹配的情感信号中学习；3)可解释、健壮、安全的MER深度学习；4)显式和隐式信号结合；5)情感理论与MER的结合。

### More Practical MER Settings

1）MER in the wild；2) MER on the edge；3) Personalized and group MER

### Real Applications Based on MER

1)在实际应用中实现MER；2)可穿戴、简单、准确的情感数据采集；3) MER的安全性、隐私性、道德性和公平性。

## conclusion

~~**不想写**~~

