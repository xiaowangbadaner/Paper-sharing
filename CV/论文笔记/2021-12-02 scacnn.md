# SCA-CNN: Spatial and Channel-wise Attention in Convolutional Networks for Image Captioning
## abstract

spatial wise和channel wise的注意力可能都不全面，所以提出了兼顾两种注意力机制的SCA-CNN

## Introduction

视觉注意可以被认为是一种动态特征提取机制。

![sca](/pic/sca.jpg)

SCA同时关注spatial和channel的特征。通道特征图本质上是相应过滤器的检测器响应图。

## related word

neural image/video captioning (NIC) and visual question answering (VQA)：使用CNN将图像或视频编码为静态视觉特征向量，然后将其输入RNN解码语言序列

* Spatial Attention
* Semantic Attention：语义选择概念，和channelwise的注意力类似。
* Multi-layer Attention：为了解决不同feature map层对应的各自字段的大小是不同。

## Spatial and Channel-wise Attention CNN

#### Overview

scA-CNN使用了多次空间和通道注意力机制

![scacnn](/pic/scacnn.jpg)

**第二行为注意力机制**，第三行用来调整权值。

为了避免GPU内存占用过多，分开估计通道注意力和空间注意力

![scacnn2](/pic/scacnn2.jpg)

#### Spatial Attention

利用单层神经网络和softmax函数生成图像区域的注意分布α。

![spatialwise](/pic/spatialwise.jpg)

#### Channel-wise Attention

通道方式应用注意机制可以看作是选择语义属性的过程

![channelwise](/pic/channelwise.jpg)

![scacnn3](/pic/scacnn3.jpg)

根据执行顺序可以有以下两种方式：

* Channel-Spatial

  ![c-s](/pic/c-s.jpg)

* Spatial-Channel

  ![s-c](/pic/s-c.jpg)

## Experiments

#### Dataset and Metric

介绍使用的数据集和评价标准

#### Setup

#### Evaluations of Channel-wise Attention 

* Comparing Methods：比较channelwise和spatialwise的效果。
* Results：VGG19具有全连接层，可以保存通道注意力。resnet152最后是全局池化，有利于保存空间注意力。层数越高，越有利于利用通道注意力机制。c-s比s-c效果更好一点。

#### Evaluations of Multi-layer Attention

* Comparing Methods：是否可以通过增加更多的注意层面来改善空间注意或渠道性注意表现。利用之前训练过的注意层权值作为初始化，这可以显著减少训练时间，比随机初始化效果更好。
* Results：层数变多效果会，但也可能导致过拟合

#### Comparison with State-of-The-Arts

* Comparing Methods ：**一般硬注意力机制比软注意力机制要好**“软”注意加权将视觉特征归纳为注意特征，而“硬”注意加权随机抽样区域特征作为注意特征。
* Results：整体模型总能比单一模型得到更好的结果

#### Visualization of Spatial and Channel-wise Attention
展示了一些定性的例子

## Conclusions

采用了空间、通道、多层注意力机制，并妄图引入时间注意力机制。