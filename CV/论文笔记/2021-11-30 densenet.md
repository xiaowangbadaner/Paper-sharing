# Densely Connected Convolutional Networks

## Abstract

如果靠近输入层和靠近输出层间存在更短的连接，则可以大大提高训练的深度、准确性和效率。

本文介绍的DenseNet以feed-forward的方式连接每一层。所以和之前的L层有L个连接不同的是，我们有L(L+1）/2 个连接。

优点：缓解了梯度消失的问题，加强了特征的传播和特征重用，减少了参数数量。

# Introduction

![densenet](/pic/densenet.jpg)

深度网络的输入信息在经过多层变换后，很多信息会消失，之前的网络为了解决这个问题，都有一个共同的关键特征:它们创建了从早期层到后期层的短路径。（resnet、highway networks、stochastic depth shortens resnet、fractalnets）

本文提出的结构，每一层从前一层获取所有额外输入，并将自己的特征图传给下一层。

resnet是将张量进行相加，而densenet是进行连接。

resnet通过恒等变换来使得前一层的信息显式的保存，densenet明确的区分了添加到网络的信息和保留的信息，最终的决策根据所有特征图得出。

densenet：1.更高的参数效率；2.改善了信息的传递和梯度的传播，更易于训练；3.具有正则化效应，可以减少过拟合。

## Related work

一个正交的方法使网络更深入是增加网络宽度(例如使用跳跃连接)。

理论上来说，只要深度足够，仅仅在每一层增加卷积核的数量就能提高模型性能。

DenseNets不是从极深或极宽的架构中获取表现能力，而是通过**特征重复利用**网络的潜力，产生易于训练和高度参数效率的密集模型。**将不同层学习到的特征图串联起来**

![densenet1](/pic/densenet1.jpg)

Network in Network (NIN)、 Deeply Supervised Network (DSN) 、Deeply-Fused Nets (DFNs)、Ladder Networks

## Densenets

 一般网络：![densenet公式](/pic/一般网络公式.jpg)

resnet：数据可以实现跳跃连接，但加和在一起可能会影响信息本来的表达。

![resnet公式](/pic/resnet公式.jpg)

**Dense connectivity：**l层会接收到[1,2,...,l-1]层的信息。

![desnet公式](/pic/desnet公式.jpg)

Composite function：定义H(l)为三个操作的组合：BN+ReLU+Conv2d

Pooling layers：利用池化操作使得特征图尺寸一致，然后可以进行拼接。我们在dense block之间的层进行卷积和池化操作，会使用BN,Conv2d,2*2pool。

Growth rate：定义K为网络的增长率，若H产生k个特征图，那么第l层有k0+k*(l-1)个输入特征，k0为输入的通道数。每一层都可以访问块中所有的前面的特征图。

Bottleneck layers：1 × 1 的卷积可以当作一个bottleneck layer放在3 × 3 的卷积之前用来减少通道数。结构为：BN-ReLU-Conv1 × 1-BN-ReLU-conv3 × 3。

Compression：在transition layer减少特征图数量（通道数）。特征图数量：n->n*k(0<k<1)

Implementation Details：使用了三个相同的dense block，在第一个denseblock前使用了16（或者原增长率的两倍）输出通道的卷积。使用了padding来保持尺寸不变。在两个块之间使用了1 × 1 的卷积和avepool来降低feature map的大小。最后使用了全局平均池化和softmax。

## experiments

* datasets：CIFAR、SVHN、ImageNet
* training：初代版本densenet可能内存效率低下，有densenet的高效实现。

## discussion

* Model compactness
* Implicit Deep Supervision：单个层通过更短的连接接受损失函数的额外监督。**深度监督的好处以前已经在深度监督网(DSN)，每个隐藏层都有分类器，强制中间层学习鉴别特征。**DenseNets以隐式的方式执行类似的深度监督:网络上的单个分类器通过最多两个或三个过渡层对所有层提供直接监督。然而，densenet的损失函数和梯度不复杂，因为所有层之间共享相同的损失函数。
* Stochastic vs. deterministic connection：如果所有中间层都被随机丢弃，那么同一池化层之间的任何两个层都有小概率被直接连接。相当于会提供正则化。
* Feature Reuse：1.从早期的层中提取的特征确实可以直接被深层所使用；2.信息会从第一层之间流到最后一次；3.过渡层可能输出许多冗余特征(平均权值较低)；4.网络后期会产生更高级的特征。

## conclusion

扩展到数百层也没有发生优化困难。

在遵循简单的连接规则的同时，DenseNets自然地整合了身份映射、深度监督和多样化深度的属性。