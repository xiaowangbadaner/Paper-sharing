#  第四节 构建WGS主流程

## 0.准备阶段

部署相关软件和工具，操作系统方面推荐linux（集群）或者Mac OS。BWA和Samtools由C编写的，安装时需要进行编译之外，另外两个只要保证系统中的java是1.8版本及以上的，那么直接下载jar包就可以使用了。

* BWA (Burrow-Wheeler Aligner)： https://github.com/lh3/bwa 这是最权威，使用最广的NGS**数据比对软件**，目前已经更新到0.7.16版本；
* Samtools：https://github.com/samtools/samtools 是一个**专门用于处理比对数据**的工具；
* Picard： http://broadinstitute.github.io/picard/ 一款强大的NGS**数据处理工具**，功能方面和Samtools有些重叠，但更多的是互补，它是由java编写的，我们直接下载最新的.jar包就行了；
* GATK：https://software.broadinstitute.org/gatk/download/ **基因数据变异检测工具**。**3.x对于绝大分部的分析需求来说是完全足够的**。我们在这里也以GATK3.8（最新版本）作为流程的重要工具进行分析流程的构建。

## 1.原始数据质控

## 2.数据预处理

![23a5c2478a2412a54bc2c48af6cfe5b](pic\23a5c2478a2412a54bc2c48af6cfe5b.png)

### 序列对比

先把这一大堆的短序列捋顺，一个个去跟该物种的**参考基因组\***比较，找到每一条read在参考基因组上的位置，然后按顺序排列好，这个过程就称为**测序数据的比对**。

**序列比对本质上是一个寻找最大公共子字符串的过程。**BWA将BW(Burrows-Wheeler)压缩算法和后缀树相结合，能够让我们以较小的时间和空间代价，获得准确的序列比对结果。

### 流程的搭建

* 为参考基因组的构建索引——这其实是在为参考序列进行Burrows Wheeler变换（wiki: 块排序压缩），以便能够在序列比对的时候进行快速的搜索和定位。同级目录下会得到` .amb .ann .bwt .pac .sa   `五个文件

	```
	$ bwa index XXX.fasta
	```

* 将reads对比至参考基因，调用的是bwa的mem比对模块。

  ```
  Usage: bwa mem [options] <idxbase> <in1.fq> [in2.fq]
  $ bwa mem -t 4 -R '@RG\tID:foo_lane\tPL:illumina\tLB:library\tSM:sample_name' /path/to/human.fasta read_1.fq.gz read_2.fq.gz > sample_name.sam
  ```
  
  * **[options]**是一系列可选的参数
  
  * **<idxbase>**要输入的是参考基因组的BW索引文件，我们上面通过bwa index构建好的那几个以human.fasta为前缀的文件便是
  
  * **<in1.fq>**和**[in2.fq]**输入的是质控后的fastq文件。输入两个是**双末端测序（Pair-End Sequencing，简称PE测序）**
  
    单末端测序Single End Sequecing，简称SE测序），即只测序其中一端。因此in2.fq是非强制性的（所以用方括号括起来），只有是双末端测序的数据时才需要添加。
  
  * **-t**，线程数，我们在这里使用4个线程
  
  * -**R** 接的是**Read Group的字符串信息**。以@RG开头，它是用来将比对的read进行分组的。不同的组之间测序过程被认为是相互独立的，这个信息对于我们后续对比对数据进行错误率分析和Mark duplicate时非常重要。在Read Group中，有如下几个信息非常重要：
  
    * ID，这是Read Group的分组ID，一般设置为测序的lane ID（不同lane之间的测序过程认为是独立的），一般都包含在fastq的文件名中。
    * PL，指的是所用的测序平台。在GATK中，PL只允许被设置为：ILLUMINA,SLX,SOLEXA,SOLID,454,LS454,COMPLETE(CG),PACBIO,IONTORRENT,CAPILLARY,HELICOS或UNKNOWN这几个信息，名字方面不区分大小写。
    * SM，样本ID，用来区分样本数据太多时测出来的不同lane。
    * LB，测序文库的名字，为了协助区分不同的group而存在。文库名字一般可以在fq文件名中找到，如果上面的lane ID足够用于区分的话，也可以不用设置LB。
  
* 为了有效节省磁盘空间，一般都会用samtools将它转化为BAM文件（SAM的特殊二进制格式），而且BAM会更加方便于后续的分析。所以我们上面比对的命令可以和samtools结合并改进为：

  ```
  $ bwa mem -t 4 -R '@RG\tID:foo_lane\tPL:illumina\tLB:library\tSM:sample_name' /path/to/human.fasta read_1.fq.gz read_2.fq.gz | samtools view -S -b - > sample_name.bam
  ```

  samtools view的-b参数指的就是输出为BAM文件，数据经过samtools转换之后我们再重定向为sample_name.bam。

### 排序

使用samtools完成

```
Usage: samtools sort [options...] [in.bam]
```

比对后得到的结果文件中，每一条记录之间位置的先后顺序是乱的，我们后续去重复等步骤都需要在比对记录按照顺序从小到大排序下来才能进行，所以这才需要进行排序

```
$ time samtools sort -@ 4 -m 4G -O bam -o sample_name.sorted.bam sample_name.bam
```

* **-@ **用于设定排序时的线程数，我们设为4

* **-m **限制排序时最大的内存消耗，这里设为4GB

* **-O** 指定输出为bam格式

* **-o** 是输出文件的名字
* 最后是输入文件：sample_name.bam

### 去除PCR重复序列（或者标记重复序列）

* PCR扩增会同时增大了变异检测结果的假阴和假阳率。主要有几个原因：
  1. DNA在打断的那一步会发生一些损失，主要表现是会引发一些碱基发生颠换变换（嘌呤-变嘧啶或者嘧啶变嘌呤），带来假的变异。PCR过程会扩大这个信号，导致最后的检测结果中混入了假的结果；
  2. PCR反应过程中也会带来新的碱基错误。发生在前几轮的PCR扩增发生的错误会在后续的PCR过程中扩大，同样带来假的变异；
  3. 对于真实的变异，PCR反应可能会对包含某一个碱基的DNA模版扩增更加剧烈（这个现象称为PCR Bias）。因此，如果反应体系是对含有reference allele的模板扩增偏向强烈，那么变异碱基的信息会变小，从而会导致假阴。

* GATK、Samtools、Platpus等这种利用贝叶斯原理的变异检测算法都是认为所用的序列数据都不是重复序列（**即将它们和其他序列一视同仁地进行变异的判断，所以带来误导**），因此必须要进行标记（去除）或者使用PCR-Free的测序方案。

* 可以利用Picard来完成这个事情

  ```
  java -jar picard.jar MarkDuplicates \ 
    REMOVE_DUPLICATES=true \
    I=sample_name.sorted.bam \
    O=sample_name.sorted.markdup.bam \
    M=sample_name.markdup_metrics.txt
  这里设置 REMOVE_DUPLICATES=true会把重复序列删除，若不加此变量，则只会把重复序列在输出的新结果中标记出来。
  ```
  **.jar在 build/libs里**

* 然后为sample_name.sorted.markdup.bam创建索引文件，它能让我们随机访问这个文件中的任意位置。

  ```
  $ samtools index sample_name.sorted.markdup.bam
  ```

### 局部重对比（Smith-Waterman算法）*

* 有时在进行之前会有一个merge操作，将同个样本的所有比对结果合并成唯一一个大的BAM文件，merge的例子如下：

	```
	$ samtools merge <out.bam> <in1.bam> [<in2.bam> ... <inN.bam>]
	```

* 局部重比对的目的是将BWA比对过程中所发现有**潜在序列插入或者序列删除（insertion和deletion，简称Indel）的区域进行重新校正**。
* <u>**这个过程往往还会把一些已知的Indel区域一并作为重比对的区域。**其**根本原因来自于参考基因组的序列特点和BWA这类比对算法本身**，注意这里不是针对BWA，而是针对所有的这类比对算法，包括bowtie等**。**这类在全局搜索最优匹配的算法在存在Indel的区域及其附近的比对情况往往不是很准确，特别是当一些存在长Indel、重复性序列的区域或者存在长串单一碱基（比如，一长串的TTTT或者AAAAA等）的区域中更是如此。**另一个重要的原因是在这些比对算法中，对碱基错配和开gap的容忍度是不同的。**具体体现在罚分矩阵的偏向上，例如，在read比对时，如果发现碱基错配和开gap都可以的话，它们会更偏向于错配。但是这种偏向错配的方式，有时候却还会反过来引起错误的开gap！这**就会导致基因组上原本应该是一个长度比较大的Indel的地方，被错误地切割成多个错配和短indel的混合集**，这必然会让我们检测到很多错误的变异。而且，这种情况还会随着所比对的read长度的增长（比如三代测序的Read，通常都有几十kbp）而变得越加严重。</u>
* Smith-Waterman算法可以极其有效地实现对全局比对结果的校正和调整，最大程度低地降低由全局比对算法的不足而带来的错误。
* GATK的局部重比对模块，除了应用这个算法之外，还会对这个区域中的read进行一次局部组装，把它们连接成为长度更大的序列，这样能够更进一步提高局部重比对的准确性。
* 使用gatk：
  * 第一步，RealignerTargetCreator ，目的是定位出所有需要进行序列重比对的目标区域（如下图）；
  * 第二步，IndelRealigner，对所有在第一步中找到的目标区域运用算法进行序列重比对，最后得到捋顺了的新结果。
  * 参数：
    * **-R **参数输入的human.fasta不是BWA比对中的索引文件前缀，而是参考基因组序列（FASTA格式）文件
    * **.vcf** 候选的重比对区除了要在样本自身的比对结果中寻找之外，还应该把人群中已知的Indel区域也包含进来，而这两个是我们在重比对过程中最常用到的。在GATK bundle ftp中下载（ftp://ftp.broadinstitute.org/bundle/）
* **当后面的变异检测必须是使用GATK，而且必须使用GATK的HaplotypeCaller模块，仅当这个时候才可以减少这个Indel局部重比对的步骤。**GATK的HaplotypeCaller中，会对潜在的变异区域进行相同的局部重比对。

### 重新校正碱基质量值（BQSR，Base Quality Score Recalibration）

* **变异检测**是一个极度依赖测序碱基质量值的步骤。因为这个质量值是衡量我们测序出来的这个碱基到底有多正确的重要（甚至是唯一）指标。

* BQSR主要是通过机器学习的方法构建测序碱基的错误率模型，然后对这些碱基的质量值进行相应的调整。

![dsadasda6](\pic\dsadasda6.webp)

* 横轴（Reported quality score）是测序结果在Base calling之后报告出来的质量值，也就是我们在FASTQ文件中看到的那些；纵轴（Empirical quality score）代表的是“真实情况的质量值（统计结果）”。

* 首先排除掉所有的已知变异位点，然后计算每个（报告出来的）质量值下面有多少个碱基在比对之后与参考基因组上的碱基是不同的，这些不同碱基就被我们认为是错误的碱基，它们的数目比例反映的就是真实的碱基错误率，换算成Phred score之后，就是纵轴的Empirical quality score了。

* BQSR的具体执行步骤如下：

  * 第一步，BaseRecalibrator，这里计算出了所有需要进行重校正的read和特征值，然后把这些信息输出为一份校准表文件（sample_name.recal_data.table）

  - 第二步，PrintReads，这一步利用第一步得到的校准表文件（sample_name.recal_data.table）重新调整原来BAM文件中的碱基质量值，并使用这个新的质量值重新输出一份新的BAM文件。

* BQSR在执行的时候是按照不同的测序lane或者测序文库来进行的，**算法就是@RG中的ID来识别各个独立的测序过程**。

## 3.变异检测

* WGS数据分析流程的一个目标——获得样本准确的变异集合。变异检测的内容一般会包括：SNP、Indel，CNV和SV等，这个流程中我们只做其中最主要的两个：**SNP和Indel。**
* 使用**GATK HaplotypeCaller**模块对样本中的变异进行检测，它也是目前最适合用于对二倍体基因组进行变异（SNP+Indel）检测的算法。
* HaplotypeCaller会先**推断**群体的单倍体组合情况，计算各个组合的几率，然后根据这些信息再**反推**每个样本的基因型组合。因此它不但特别适合应用到群体的变异检测中，而且还能够依据群体的信息更好地计算每个个体的变异数据和它们的基因型组合。
* WGS流程中对HaplotypeCaller的应用有两种做法，差别只在于要不要在中间生成一个gVCF：
  * 第一种，直接进行HaplotypeCaller，这适合于**单样本**，或者**固定样本数量**的情况。（N+1难题）
  * 第二种，每个样本先各自生成gVCF(genome VCF)，然后再进行群体joint-genotype。为了解决N+1，是每个样本用于变异检测的中间文件，格式类似于VCF，它把joint-genotype过程中所需的所有信息都记录在这里面。这样一旦新增加样本也不需要再重新去读取所有人的BAM文件了，只需为新样本生成一份gVCF，然后重新执行这个joint-genotype就行了。

### 变异检测质控和过滤（VQSR）

VQSR是通过构建GMM模型对好和坏的变异进行区分，从而实现对变异的质控。

